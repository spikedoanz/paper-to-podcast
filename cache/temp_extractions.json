{"data": [[[{"topic": "Introduction", "subtopics": ["Background"]}, {"topic": "Model Architecture", "subtopics": ["Encoder and Decoder Stacks", "Attention", "Position-wise Feed-Forward Networks", "Embeddings and Softmax", "Positional Encoding"]}, {"topic": "Training", "subtopics": ["Training Data and Batching", "Hardware and Schedule", "Optimizer", "Regularization"]}, {"topic": "Results", "subtopics": ["Machine Translation", "Model Variations"]}], 4, 12, 10.0, 0], [[{"topic": "Beam Search", "subtopics": ["Beam size of 4", "Length penalty \u03b1 = 0.6", "Terminating early when possible"]}, {"topic": "Model Variations", "subtopics": ["Varying base model", "Measuring performance on English-to-German translation"]}, {"topic": "English Constituency Parsing", "subtopics": ["Challenges of the task", "Training a 4-layer transformer", "Results compared to previous models"]}, {"topic": "Conclusion", "subtopics": ["Introduction of the Transformer", "Achieving state-of-the-art performance", "Plans for future applications of attention-based models"]}], 4, 11, 9.5, 1]]}